{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeed9b3b-dd7e-4c3f-b37a-f3c560c5f1a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "secret=dbutils.secrets.get('databricklogin','databricklogin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f60faed2-a26d-42bd-b674-f1b3062d7d22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "client_id =\"88dec17f-62f0-4cab-82b7-b9022c2cc21c\"\n",
    "client_secret=secret\n",
    "directory_id=\"037e0179-cd0e-4201-951c-167b7554d77a\"\n",
    "\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": f\"{client_id}\",\n",
    "           \"fs.azure.account.oauth2.client.secret\": f\"{client_secret}\",\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8fbb5c7d-a901-47c5-b630-fcd415f37c07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/httpforstorage/raw has been unmounted.\nUnmounted existing mount at /mnt/httpforstorage/raw\nMounted successfully at/mnt/httpforstorage/raw\n"
     ]
    }
   ],
   "source": [
    "mount_point = \"/mnt/httpforstorage/raw\"\n",
    "\n",
    "if any(mount.mountPoint == mount_point for mount in dbutils.fs.mounts()):\n",
    "    # Unmount the existing mount point\n",
    "    dbutils.fs.unmount(mount_point)\n",
    "    print(f\"Unmounted existing mount at {mount_point}\")\n",
    "\n",
    "try:\n",
    "    dbutils.fs.mount(\n",
    "        source=\"abfss://raw@httpforstorage.dfs.core.windows.net/\",\n",
    "        mount_point=mount_point,\n",
    "        extra_configs=configs\n",
    "    )\n",
    "    print(f\"Mounted successfully at{mount_point}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error mounting: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3ddda618-afb9-4797-a212-3333e96ae4d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>dbfs:/mnt/httpforstorage/raw/ExpandedPatientData202412061539.csv</td><td>ExpandedPatientData202412061539.csv</td><td>9308</td><td>1733499605000</td></tr><tr><td>dbfs:/mnt/httpforstorage/raw/ExpandedTestResults202412061540.csv</td><td>ExpandedTestResults202412061540.csv</td><td>3896</td><td>1733499617000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "dbfs:/mnt/httpforstorage/raw/ExpandedPatientData202412061539.csv",
         "ExpandedPatientData202412061539.csv",
         9308,
         1733499605000
        ],
        [
         "dbfs:/mnt/httpforstorage/raw/ExpandedTestResults202412061540.csv",
         "ExpandedTestResults202412061540.csv",
         3896,
         1733499617000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"/mnt/httpforstorage/raw\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c5af410-1a00-4f67-8b8d-f6c9bc00c2cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "application_id =\"88dec17f-62f0-4cab-82b7-b9022c2cc21c\"\n",
    "service_credential=\"idG8Q~KkSdwEBRYgavLJ2DkzuYElGDbi2L3pIbjm\"\n",
    "directory_id=\"037e0179-cd0e-4201-951c-167b7554d77a\"\n",
    "spark.conf.set(\"fs.azure.account.auth.type.httpforstorage.dfs.core.windows.net\", \"OAuth\")\n",
    "spark.conf.set(\"fs.azure.account.oauth.provider.type.httpforstorage.dfs.core.windows.net\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\")\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.id.httpforstorage.dfs.core.windows.net\", application_id)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.secret.httpforstorage.dfs.core.windows.net\", service_credential)\n",
    "spark.conf.set(\"fs.azure.account.oauth2.client.endpoint.httpforstorage.dfs.core.windows.net\", f\"https://login.microsoftonline.com/{directory_id}/oauth2/token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bdc3a155-4635-48a7-9d7a-3a50cc7ac608",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>path</th><th>name</th><th>size</th><th>modificationTime</th></tr></thead><tbody><tr><td>abfss://raw@httpforstorage.dfs.core.windows.net/ExpandedPatientData202412061539.csv</td><td>ExpandedPatientData202412061539.csv</td><td>9308</td><td>1733499605000</td></tr><tr><td>abfss://raw@httpforstorage.dfs.core.windows.net/ExpandedTestResults202412061540.csv</td><td>ExpandedTestResults202412061540.csv</td><td>3896</td><td>1733499617000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "abfss://raw@httpforstorage.dfs.core.windows.net/ExpandedPatientData202412061539.csv",
         "ExpandedPatientData202412061539.csv",
         9308,
         1733499605000
        ],
        [
         "abfss://raw@httpforstorage.dfs.core.windows.net/ExpandedTestResults202412061540.csv",
         "ExpandedTestResults202412061540.csv",
         3896,
         1733499617000
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "path",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "size",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modificationTime",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dbutils.fs.ls(\"abfss://raw@httpforstorage.dfs.core.windows.net\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07a425a5-7114-48c4-91d4-9fb584091253",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n|Unique Patients|\n+---------------+\n|            100|\n+---------------+\n\n+------------+\n|Unique Tests|\n+------------+\n|         100|\n+------------+\n\nCleaned patient data saved to abfss://processed@httpforstorage.dfs.core.windows.net/CleanedPatientData\nCleaned test data saved to abfss://processed@httpforstorage.dfs.core.windows.net/CleanedTestResults\nCleaned data saved to /mnt/httpforstorage/processed/CleanedPatientData and /mnt/httpforstorage/processed/CleanedTestResults using Delta Lake\n"
     ]
    }
   ],
   "source": [
    "# Databricks Data Cleaning for COVID-19 Data\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, lit, countDistinct\n",
    "from pyspark.sql.types import StringType, IntegerType, DateType\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"COVIDDataCleaning\").getOrCreate()\n",
    "\n",
    "# Load Patient Data\n",
    "patient_file_path = \"/mnt/httpforstorage/raw/ExpandedPatientData202412061539.csv\"  # Update path if needed\n",
    "patients_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(patient_file_path)\n",
    "\n",
    "# Load Test Data\n",
    "test_file_path = \"/mnt/httpforstorage/raw/ExpandedTestResults202412061540.csv\"  # Update path if needed\n",
    "tests_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(test_file_path)\n",
    "\n",
    "# --- Data Cleaning on Patients Data ---\n",
    "# Drop rows with missing PatientID\n",
    "patients_df = patients_df.filter(col(\"PatientID\").isNotNull())\n",
    "\n",
    "# Cast PatientID and Age to Integer and TestDate to Date\n",
    "def clean_patient_data(df):\n",
    "    return (df.withColumn(\"PatientID\", col(\"PatientID\").cast(IntegerType()))\n",
    "              .withColumn(\"Age\", col(\"Age\").cast(IntegerType()))\n",
    "              .withColumn(\"TestDate\", col(\"TestDate\").cast(DateType())))\n",
    "\n",
    "patients_df = clean_patient_data(patients_df)\n",
    "\n",
    "\n",
    "patients_df = patients_df.withColumn(\"COVIDStatus\", when(col(\"COVIDStatus\").isNull(), lit(\"Unknown\"))\n",
    "                                     .otherwise(col(\"COVIDStatus\")))\n",
    "\n",
    "\n",
    "tests_df = tests_df.filter(col(\"TestID\").isNotNull() & col(\"PatientID\").isNotNull())\n",
    "\n",
    "\n",
    "def clean_test_data(df):\n",
    "    return (df.withColumn(\"TestID\", col(\"TestID\").cast(IntegerType()))\n",
    "              .withColumn(\"PatientID\", col(\"PatientID\").cast(IntegerType()))\n",
    "              .withColumn(\"ResultDate\", col(\"ResultDate\").cast(DateType())))\n",
    "\n",
    "tests_df = clean_test_data(tests_df)\n",
    "\n",
    "\n",
    "tests_df = tests_df.withColumn(\"LabName\", when(col(\"LabName\").isNull(), lit(\"Unknown\"))\n",
    "                               .otherwise(col(\"LabName\")))\n",
    "\n",
    "patients_df.select(countDistinct(\"PatientID\").alias(\"Unique Patients\")).show()\n",
    "tests_df.select(countDistinct(\"TestID\").alias(\"Unique Tests\")).show()\n",
    "\n",
    "# Save cleaned data as Parquet files and use Delta Lake for appending\n",
    "cleaned_patient_file_path = \"/mnt/httpforstorage/processed/CleanedPatientData\"\n",
    "cleaned_test_file_path = \"/mnt/httpforstorage/processed/CleanedTestResults\"\n",
    "cleaned_patient_output_path = \"abfss://processed@httpforstorage.dfs.core.windows.net/CleanedPatientData\"\n",
    "cleaned_test_output_path = \"abfss://processed@httpforstorage.dfs.core.windows.net/CleanedTestResults\"\n",
    "\n",
    "patients_df.write.format(\"delta\").mode(\"append\").save(cleaned_patient_file_path)\n",
    "tests_df.write.format(\"delta\").mode(\"append\").save(cleaned_test_file_path)\n",
    "patients_df.write.format(\"delta\").mode(\"append\").save(cleaned_patient_output_path)\n",
    "print(f\"Cleaned patient data saved to {cleaned_patient_output_path}\")\n",
    "\n",
    "tests_df.write.format(\"delta\").mode(\"append\").save(cleaned_test_output_path)\n",
    "print(f\"Cleaned test data saved to {cleaned_test_output_path}\")\n",
    "\n",
    "print(f\"Cleaned data saved to {cleaned_patient_file_path} and {cleaned_test_file_path} using Delta Lake\")\n",
    "tests_df.show()\n",
    "patients_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c9ae962-cac8-4a10-9e6e-b5cc6d3317f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+-----------+------+-----------------+----------+------------+\n|PatientID|Age|COVIDStatus|TestID|          LabName|ResultDate|ResultStatus|\n+---------+---+-----------+------+-----------------+----------+------------+\n|        1| 20|   Negative|  1035|          LabCorp|2024-06-11|    Negative|\n|        2| 33|  Recovered|  1049|         PathCare|2024-11-07|    Negative|\n|        3| 42|   Negative|  1023|Quest Diagnostics|2024-10-08|    Negative|\n|        4| 61|   Negative|  1075|     MedLife Labs|2024-08-15|    Negative|\n|        5| 34|  Recovered|  1078|Quest Diagnostics|2024-09-08|    Negative|\n|        6| 44|   Negative|  1063|         PathCare|2024-08-04|    Negative|\n|        7| 65|  Recovered|  1066|          LabCorp|2024-09-08|    Negative|\n|        9| 59|   Positive|  1095|          LabCorp|2024-01-27|    Positive|\n|       10| 46|   Positive|  1058|Quest Diagnostics|2024-05-29|    Positive|\n|       12| 26|   Negative|  1094|     MedLife Labs|2024-05-15|    Negative|\n|       13| 35|  Recovered|  1046|          LabCorp|2024-11-17|    Negative|\n|       15| 62|  Recovered|  1001|     MedLife Labs|2024-07-27|    Negative|\n|       16| 56|  Recovered|  1039|         PathCare|2024-09-27|    Negative|\n|       19| 64|   Negative|  1014|Quest Diagnostics|2024-09-13|    Negative|\n|       20| 66|   Positive|  1084|         PathCare|2024-10-09|    Positive|\n|       21| 20|  Recovered|  1029|         PathCare|2024-07-24|    Negative|\n|       22| 25|   Negative|  1028|         PathCare|2024-11-23|    Negative|\n|       23| 39|   Positive|  1074|     MedLife Labs|2024-02-14|    Positive|\n|       25| 65|  Recovered|  1097|          LabCorp|2024-06-08|    Negative|\n|       26| 65|   Positive|  1089|     MedLife Labs|2024-08-07|    Positive|\n+---------+---+-----------+------+-----------------+----------+------------+\nonly showing top 20 rows\n\nETL output data saved to abfss://gold@httpforstorage.dfs.core.windows.net/ETLResultData\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "patients_df = spark.read.format(\"delta\").load(cleaned_patient_output_path)\n",
    "tests_df = spark.read.format(\"delta\").load(cleaned_test_output_path)\n",
    "\n",
    "# Join the cleaned patient and test data on PatientID\n",
    "joined_df = patients_df.join(tests_df, on=\"PatientID\", how=\"inner\")\n",
    "\n",
    "# Add a new column to indicate if a patient's test result is 'Positive' or 'Negative'\n",
    "joined_df = joined_df.withColumn(\"ResultStatus\", when(col(\"COVIDStatus\") == \"Positive\", lit(\"Positive\"))\n",
    "                                               .otherwise(lit(\"Negative\")))\n",
    "\n",
    "# Select and transform relevant columns for reporting\n",
    "etl_output_df = joined_df.select(\n",
    "    col(\"PatientID\"),\n",
    "    col(\"Age\"),\n",
    "    col(\"COVIDStatus\"),\n",
    "    col(\"TestID\"),\n",
    "    col(\"LabName\"),\n",
    "    col(\"ResultDate\"),\n",
    "    col(\"ResultStatus\")\n",
    ")\n",
    "\n",
    "# Show a preview of the transformed data\n",
    "etl_output_df.show()\n",
    "\n",
    "# Save the transformed data to Delta format in Azure Storage\n",
    "etl_output_path = \"abfss://gold@httpforstorage.dfs.core.windows.net/ETLResultData\"\n",
    "\n",
    "etl_output_df.write.format(\"delta\").mode(\"append\").save(etl_output_path)\n",
    "print(f\"ETL output data saved to {etl_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a5c65db-f03e-4fad-b67d-dd743acd4c9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['PatientID', 'Name', 'Email', 'Phone', 'Country', 'COVIDStatus', 'Age', 'TestDate']\n"
     ]
    }
   ],
   "source": [
    "print(patients_df.columns)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "ETL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}